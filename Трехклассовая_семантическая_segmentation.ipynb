{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RbBwpCscHK3"
      },
      "source": [
        "# Задание 2: Трехклассовая семантическая сегментация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EozEBa0QcHK7"
      },
      "source": [
        "Предлагается решить задачу семантической сегментации животных с тремя классами: класс \"фон\" (метка 0), класс \"кошка\" (метка 1) и класс \"собака\" (метка 2).\n",
        "![Image](https://miro.medium.com/max/1130/1*DDEkOFC93pEbrTdyhdpXZg.png)\n",
        "\n",
        "Для этого сами подготовим [датасет](https://drive.google.com/uc?export=download&id=1ZsRAXiPgOU5Am8tNZ7mruwtJh3ck8TI5), реализуем метрики/функции потерь, реализуем и обучим свою [PSPNet](https://arxiv.org/abs/1612.01105)-подобную архитектуру."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNPQVbUDlEXY",
        "outputId": "a54ef447-58b8-486c-e69a-4c2dd3b5845c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmmxRugUcHK7"
      },
      "source": [
        "### Загрузка модулей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIHT-w6ycHK8"
      },
      "outputs": [],
      "source": [
        "# Загружаем pytorch для работы с нейронными сетями\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Для работы с изображениями/графиками\n",
        "from torchvision import transforms\n",
        "# Загружаем способы интерполяции изображений\n",
        "from torchvision.transforms.functional import InterpolationMode as IM\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Для логирования метрик и функций потерь в ходе обучения\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Для удобной работы с обучающей/тестовой выборкой\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Прочее\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHCngWRkcHK9"
      },
      "source": [
        "## Часть 1: Подготовка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lNk8FaLcHK-"
      },
      "source": [
        "### 1.1 Предобработка датасета (1 балл)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "333j6m2WcHK-"
      },
      "source": [
        "Для начала работы с данными требуется выполнить следующие пункты:\n",
        "- Определиться со способом хранения/чтения данных с диска. В задачах комьютерного зрения датасеты, как правило, имеют большой размер, который не помещается в оперативную память. Поэтому предлагается несколько форматов хранения: `HDF5`, `memory-mapped files` и `\"сырой\"` вид, т.е хранение `.jpg/.png` файлов на диске. Классы с указанными способами уже описаны в файле `utils.py`. Вам предлагается лишь замерить скорость чтения данных для каждого из форматов, затем выбрать наиболее быстрый (чуть позже).\n",
        "    - Поговорим поподробнее об особенностях этих форматов хранения. Формат `hdf5` позволяет разбивать массивы информации на [chunks](https://www.oreilly.com/library/view/python-and-hdf5/9781491944981/ch04.html), которые организованы в виде B-деревьев. Это имеет смысл при чтении `hyperslabs` - многомерных срезов массива, которые несмежны в памяти (non-contiguous). По умолчанию, `hdf5` хранит данные непрерывно (contiguous)\n",
        "    - `Memory-mapping` файлов в оперативную память позволяет пропустить этап буфферизации, тем самым пропуская операцию копирования, лениво загружая информацию напрямую. Особенность этого подхода в том, что алгоритмически `Best case` скорости чтения достигается на непрерывном блоке информации (contiguous), а `Worst case` - наоборот, на несмежном в памяти (non-contiguous) блоке (на порядки хуже, чем потенциально возможно в `hdf5`).\n",
        "\n",
        "- Привести все пары (изображение, маска) к единому размеру `target_shape`, указанному далее в словаре конфигурации `default_config`. Предлагается следующая последовательность действий:\n",
        "    1. При помощи [transforms.Resize](https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html#torchvision.transforms.Resize) мы можем интерполировать (по умолчанию билинейная интерполяция) значения пикселей при изменении размера исходного изображения до заданного. Однако, подобная операция искажает исходное соотношение сторон изображения, что может негативно сказаться на предсказательной способности сети. Например, общий вид морды кошки будет зависеть от исходного размера изображения, а не от сущности класса \"кошка\": оно может быть не растянуто, может быть растянуто вертикально/горизонтально. Неконсистентность в представлении одной и той же сущности может привести к нестабильному обучению, так как размеры ядра свертки едины для любого входного изображения! К счастью, эта проблема уже решена в `transforms.Resize`: при целочисленном аргументе `size` наименьшая сторона входного изображения будет интерполирована до `size`, а другая сторона (наибольшая) до размера `size * aspect_ratio`, т.е сохраняя соотношение сторон `aspect_ratio`\n",
        "    2. На текущий момент лишь одна из сторон исходного изображения соответствует требуемому размеру `target_shape`. Возможны два случая: оставшаяся сторона меньше или больше (случай с равенством можно свести к ситуации \"меньше на 0\") требуемого размера. В первом случае будем дополнять изображение пикселями со значением `pad_value` при помощи [transforms.Pad](https://pytorch.org/vision/stable/generated/torchvision.transforms.Pad.html#torchvision.transforms.Pad), а во втором - обрезать изображение при помощи [transforms.CenterCrop](https://pytorch.org/vision/stable/generated/torchvision.transforms.CenterCrop.html#torchvision.transforms.CenterCrop).\n",
        "\n",
        "> Последовательное исполнение операций модуля `transforms` можно выполнить при помощи [transforms.Compose](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html).\n",
        "- Ответить на вопрос: `А зачем, вообще, требуется сводить все изображения к одному размеру?`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6-bQDQkcHK_"
      },
      "source": [
        "Ваш ответ: нейросеть рассчитана на определенный размер изображений, т.к. мы указываем в параметрах для матричных операций и слоев размерность изображений."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlLgwN2YcHLA"
      },
      "outputs": [],
      "source": [
        "from utils import *\n",
        "\n",
        "def resize(img: Image.Image, target_shape: Tuple[int, int], pad_val: int) -> np.array:\n",
        "    \"\"\"\n",
        "    Приводит входное изображение `img` к размеру `target_shape`, указанной выше\n",
        "    последовательностью действий. Предполагается, что требуемый размер `target_shape` \"квадратный\"\n",
        "    \"\"\"\n",
        "    # Проверяем равенство желаемых размеров сторон изображения\n",
        "    assert target_shape[0] == target_shape[1]\n",
        "\n",
        "    # Необходимо для \"универсальности\" определения количества недостающих padding-пикселей.\n",
        "    # В случае если размерность текущего изображения меньше требуемой - имеем неотрицательное\n",
        "    # Количество недостающих padding-пикселей, relu возвращает это число без изменений.\n",
        "    # В случае если размерность текущего изображения больше требуемой - имеем отрицательное\n",
        "    # Количество недостающих padding-пикселей, т.е в padding пикселях мы не нуждаемся и\n",
        "    # relu возвращает значение 0.\n",
        "    def relu(x):\n",
        "        return x * (x > 0)\n",
        "\n",
        "    # Масштабируем наименьшую размерность `img` под `target_shape`\n",
        "    # В качестве способа интерполяции выберем интерполяцию методом ближайшего соседа\n",
        "    # Это необходимо для сохранения множества значений маски сегментации\n",
        "    img = transforms.Resize(target_shape[0], interpolation=IM.NEAREST)(img)\n",
        "\n",
        "    # Вычисляем количество недостающих padding-пикселей для каждой из сторон изображения\n",
        "    h, w = img.size[0], img.size[1]\n",
        "    h_diff, w_diff = target_shape[0] - h, target_shape[1] - w\n",
        "\n",
        "    pad_left, pad_right = w_diff // 2, w_diff - (w_diff // 2)\n",
        "    pad_top, pad_bottom = h_diff // 2, h_diff - (h_diff // 2)\n",
        "\n",
        "    resize_transform = transforms.Compose([\n",
        "        # Добавляем padding-пиксели. Если их нет, то операция Pad ничего не изменит (случай \"больше\").\n",
        "        transforms.Pad(\n",
        "            (relu(pad_left), relu(pad_top), relu(pad_right), relu(pad_bottom)),\n",
        "            fill=pad_val, padding_mode=\"constant\"\n",
        "        ),\n",
        "        # Обрезаем \"лишние\" пиксели. Если их нет, то CenterCrop ничего не изменит (случай \"меньше\").\n",
        "        transforms.CenterCrop(target_shape[0]),\n",
        "\n",
        "        # Преобразуем PIL.Image изображение в массив np.array\n",
        "        transforms.Lambda(lambda x: np.array(x))\n",
        "    ])\n",
        "\n",
        "    return resize_transform(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o03ijFvrcHLB"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(config: dict, storage_class: Type[storage_class]):\n",
        "    \"\"\"\n",
        "    Предобрабатывает датасет и эффективно его сохраняет на диск\n",
        "    \"\"\"\n",
        "    with open(config[\"annotation_file\"]) as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Заводим массивы для блоков изображений, помещаемых в память\n",
        "    input_chunk = np.empty((config[\"chunk_size\"], *config[\"target_shape\"], 3), dtype=np.uint8)\n",
        "    target_chunk = np.empty((config[\"chunk_size\"], *config[\"target_shape\"]), dtype=np.uint8)\n",
        "\n",
        "    # Делим датасет на блоки\n",
        "    config[\"dataset_size\"] = len(lines)\n",
        "    num_chunks = config[\"dataset_size\"] // config[\"chunk_size\"] + bool(config[\"dataset_size\"] % config[\"chunk_size\"])\n",
        "    dataset = storage_class(config)\n",
        "\n",
        "    # Читаем изображения с диска, предобрабатываем и сохраняем в выбранный нами формат\n",
        "    for chunk_idx in tqdm(range(num_chunks)):\n",
        "        print(\"Start\")\n",
        "        for pos in range(config[\"chunk_size\"]):\n",
        "            flat_idx = chunk_idx * config[\"chunk_size\"] + pos\n",
        "            if (flat_idx >= config[\"dataset_size\"]):\n",
        "                break\n",
        "\n",
        "            img_name, label = lines[flat_idx].rstrip(\"\\n\").split(' ')\n",
        "\n",
        "            # print('input')\n",
        "            input_raw = Image.open(os.path.join(config[\"input_dir\"], img_name + \".jpg\")).convert(\"RGB\")\n",
        "            # print('targer')\n",
        "            target_raw = Image.open(os.path.join(config[\"target_dir\"], img_name + \".png\")).convert('L')\n",
        "            # print('next')\n",
        "\n",
        "            input_chunk[pos] = resize(input_raw, config[\"target_shape\"], 0)\n",
        "            target_chunk[pos] = renumerate_target(resize(target_raw, config[\"target_shape\"], 2), int(label))\n",
        "        dataset.append(input_chunk, target_chunk)\n",
        "    dataset.lock()\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbCD4wQbcHLC"
      },
      "source": [
        "Для простоты будем выбирать размер изображений `target_shape` с одинаковыми сторонами. Предлагается использовать размер `256x256`, хотя выбор за вами. Обратите внимание, что от размера изображений зависит быстродействие дальнейшего кода (чем больше картинки, тем дольше обучать)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "os.remove('/content/drive/MyDrive/SegTask/trainval.h5')\n",
        "shutil.rmtree('/content/drive/MyDrive/SegTask/trainval')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "htVix_BdTfhT",
        "outputId": "576f0fe2-55f1-4eff-cbd4-cb6de743c7e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0812298750d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/SegTask/trainval.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/SegTask/trainval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# lstat()/open()/fstat() trick.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m             \u001b[0morig_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/SegTask/trainval'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I89HKu0scHLC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429,
          "referenced_widgets": [
            "49f33d7bf55c439cbdd37c1f10bab439",
            "8a44f67409424ca291f50cfe02e4770e",
            "89d3d1f29b564a10bd0747e8f30a6f04",
            "c18c8b4c00f44b0c8ed74fd9324be686",
            "b313786dc33341739d71dd85d95b1625",
            "fb9d93da51e3458d9cf80985d239db06",
            "0b9eeb5ab4624ec39e8d18b6bbcb7888",
            "14eee69c1bcf4430892ee14289e09132",
            "eb6401b48b254c6590ce91e799df1d20",
            "d4393105203d489cac0a2854efddac94",
            "ea207ac9769a47e79bdded2974a3ae94"
          ]
        },
        "outputId": "811f0a8f-930b-4c77-f85b-1fdf77a7d6cc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49f33d7bf55c439cbdd37c1f10bab439"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-3b4329d8c3c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mconfig_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"annotation_file\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/SegTask/test.txt\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdefault_config\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_data_hdf5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_hdf5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_hdf5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtrain_data_memmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_memmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-cf18a82a6e1c>\u001b[0m in \u001b[0;36mprepare_dataset\u001b[0;34m(config, storage_class)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# print('input')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0minput_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_dir\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;31m# print('targer')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtarget_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target_dir\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2850\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2852\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Конфигурация датасета\n",
        "default_config = {\n",
        "             \"input_dir\": \"/content/drive/MyDrive/SegTask/images/\",\n",
        "             \"target_dir\": \"/content/drive/MyDrive/SegTask/seg_masks/\",\n",
        "             \"target_shape\": (256, 256), # Можно любой другой размер картинки\n",
        "             \"chunk_size\": 512, # количество изображений в блоке, загружаемых в оперативную память\n",
        "            }\n",
        "\n",
        "# Конфигурации обучающей и тестовой выборок отличаются файлов аннотации\n",
        "config_train = {**{\"annotation_file\": \"/content/drive/MyDrive/SegTask/trainval.txt\"},  **default_config}\n",
        "config_test = {**{\"annotation_file\": \"/content/drive/MyDrive/SegTask/test.txt\"}, **default_config}\n",
        "\n",
        "train_data_hdf5 = prepare_dataset(config_train, storage_hdf5)\n",
        "print(type(train_data_hdf5))\n",
        "train_data_memmap = prepare_dataset(config_train, storage_memmap)\n",
        "print(type(train_data_memmap))\n",
        "train_data_raw = prepare_dataset(config_train, storage_raw)\n",
        "print(type(train_data_raw))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyyQ0gFqcHLD"
      },
      "source": [
        "### 1.2 Создание Dataset и DataLoader (1.5 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rmRp6zccHLD"
      },
      "source": [
        "Pytorch [предоставляет](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) нам удобные обертки Dataset и DataLoader для наших данных, которые эффективно нарезают наш датасет на `batches` (блоки) заданного размера, а также параллелизуют процесс чтения на `num_workers` нитей.\n",
        "\n",
        "Также для дальнейшей работы нам понадобится [аугментация](https://pytorch.org/vision/stable/transforms.html) данных. Ее цель заключается в еще большем расширении обучающей выборки путем применения преобразований над изображениями, которые изменяют их абсолютные значения пикселей, но не нарушают их информационное наполнение.\n",
        "\n",
        "Например, преобразование [ColorJitter](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html#torchvision.transforms.ColorJitter) способно изменить яркость изображения на случайное число, что не изменяет его контекст. Однако, преобразование  [RandomCrop](https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop) не рекомендуется, посколько есть шанс, что мордочка животного не попадет в фото и класс животного будет неоднозначен. Таким образом, при каждом вызове объекта из обучающей выборки к нему будет применяться случайное преобразование/серия случайных преобразований. `Обратите внимание, что преобразование изображения должно быть согласованным с его сегментационной маской`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tREphN17cHLD"
      },
      "source": [
        "Требуется реализовать предлагаемые ниже преобразования аугментации:\n",
        "- `HorizontalFlip` (0.25 балла)\n",
        "- `ColorJitter` (0.25 балла)\n",
        "- `RandomPerspective` (0.5 балла)\n",
        "\n",
        "Для каждого из указанных преобразований требуется написать магический метод `__call__`, который позволяет обращаться к объекту класса (преобразованию), как к функции (функтор из C++):\n",
        "```Python\n",
        "# инициализация\n",
        "obj = Example()\n",
        "# вызывается __call__\n",
        "obj()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDq0sm3zcHLE"
      },
      "outputs": [],
      "source": [
        "# Предлагается использовать эти функции\n",
        "# Самому писать процедуры отражения картинки по вертикали/горизонтали или цветокоррекции не надо!\n",
        "from torchvision.transforms.functional import hflip\n",
        "from torchvision.transforms.functional import perspective\n",
        "from torchvision.transforms import ColorJitter as CJ\n",
        "\n",
        "\n",
        "class HorizontalFlip():\n",
        "    def __init__(self, prob: float):\n",
        "        self.p = prob\n",
        "\n",
        "    def __call__(self, pair: Tuple[Image.Image, Image.Image]) -> Tuple[Image.Image, Image.Image]:\n",
        "        \"\"\"\n",
        "        `pair` содержит пару (изображение, сегментационная маска)\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        ==== YOUR CODE =====\n",
        "             ¯\\_(ツ)_/¯\n",
        "        \"\"\"\n",
        "        pair = Tuple[hflip(pair[0]), hflip(pair[1])]\n",
        "        return pair\n",
        "\n",
        "\n",
        "class ColorJitter():\n",
        "    def __init__(self, prob: float, param: Tuple[float]):\n",
        "        self.p = prob\n",
        "        \"\"\"\n",
        "        ==== YOUR CODE =====\n",
        "             ¯\\_(ツ)_/¯\n",
        "        \"\"\"\n",
        "        self.CJ = CJ(*param)\n",
        "\n",
        "    def __call__(self, pair: Tuple[Image.Image, Image.Image]) -> Tuple[Image.Image, Image.Image]:\n",
        "        \"\"\"\n",
        "        `pair` содержит пару (изображение, сегментационная маска)\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        ==== YOUR CODE =====\n",
        "             ¯\\_(ツ)_/¯\n",
        "        \"\"\"\n",
        "\n",
        "        pair = Tuple[CJ(pair[0]), CJ(pair[1])]\n",
        "        return pair\n",
        "\n",
        "\n",
        "class RandomPerspective():\n",
        "    def __init__(self, prob: float, param: float):\n",
        "        self.p = prob\n",
        "        \"\"\"\n",
        "        ==== YOUR CODE =====\n",
        "             ¯\\_(ツ)_/¯\n",
        "        \"\"\"\n",
        "        self.param = param\n",
        "\n",
        "    def __call__(self, pair: Tuple[Image.Image, Image.Image]) -> Tuple[Image.Image, Image.Image]:\n",
        "        \"\"\"\n",
        "        `pair` содержит пару (изображение, сегментационная маска)\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        ==== YOUR CODE =====\n",
        "             ¯\\_(ツ)_/¯\n",
        "        \"\"\"\n",
        "        pair = Tuple[perspective(pair[0], self.param), perspective(pair[1], self.param)]\n",
        "        return pair"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqwR7fhdcHLE"
      },
      "source": [
        "Применим реализованные преобразования и убедимся в их работоспособности:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR14l1GLcHLE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 705
        },
        "outputId": "eac35d21-d231-42f0-9b42-00bea6d3d8ea"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b0334c60d0f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_hdf5\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m imgs2draw = {\"Source\": pair,\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data_hdf5' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x576 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "img_idx = np.random.randint(0, 100)\n",
        "f, ax = plt.subplots(2, 4, figsize=(16, 8))\n",
        "pair = train_data_hdf5[img_idx]\n",
        "\n",
        "imgs2draw = {\"Source\": pair,\n",
        "            \"HorizontalFlip\": HorizontalFlip(1.0)(pair),\n",
        "            \"ColorJitter\": ColorJitter(1.0, (0.4, 0.4, 0.4))(pair),\n",
        "            \"RandomPerspective\": RandomPerspective(1.0, 0.25)(pair)\n",
        "}\n",
        "for idx, (name, pair) in enumerate(imgs2draw.items()):\n",
        "    ax[0, idx].imshow(pair[0])\n",
        "    ax[0, idx].set_title(name, fontsize=20)\n",
        "    ax[1, idx].imshow(colorize(np.array(pair[1])))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmXHbaVrcHLF"
      },
      "source": [
        "Далее описываем наш класс `SegmentationData` и операции приведения изображений типа PIL.Image к pytorch тензорам с ImageNet `нормализацией`. ImageNet нормализация - это частный случай [Standard normalization](https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html), в котором поканальное среднее (цветовые каналы red, green, blue) и поканальное среднеквадратическое отклонение вычислены на [огромной выборке изображений](https://en.wikipedia.org/wiki/ImageNet).\n",
        "\n",
        "Ответьте на вопрос: `А для чего нужно применять нормализацию к изображениям?`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHA_6CoxcHLF"
      },
      "source": [
        "Ваш ответ: чтобы при преобразованиях и вычислений результаты были меньше и не произошло переполнения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyfHcNuVcHLF"
      },
      "outputs": [],
      "source": [
        "# Определяем устройство для вычислений (!желательно GPU!)\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "t_dict = {\n",
        "    \"forward_input\": transforms.Compose([\n",
        "        transforms.PILToTensor(),\n",
        "        transforms.Lambda(lambda x: x.float().to(DEVICE)/255.0),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    \"backward_input\": transforms.Compose([\n",
        "        transforms.Normalize(mean=[0.0, 0.0, 0.0],\n",
        "                                     std=[1./0.229, 1./0.224, 1./0.225]),\n",
        "        transforms.Normalize(mean=[-0.485, -0.456, -0.406],\n",
        "                                     std=[1.0, 1.0, 1.0]),\n",
        "        transforms.Lambda(lambda x: x.permute(1, 2, 0).cpu().numpy())\n",
        "    ]),\n",
        "    \"forward_target\": transforms.Compose([\n",
        "        transforms.PILToTensor(),\n",
        "        transforms.Lambda(lambda x: x.long().squeeze().to(DEVICE)),\n",
        "    ]),\n",
        "    \"backward_target\": transforms.Compose([\n",
        "        transforms.Lambda(lambda x: x.cpu().numpy())\n",
        "    ]),\n",
        "    \"augment\": transforms.Compose([\n",
        "        HorizontalFlip(0.5),\n",
        "        ColorJitter(0.5, (0.4, 0.4, 0.4)),\n",
        "        RandomPerspective(0.5, 0.25)\n",
        "    ]),\n",
        "}\n",
        "\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, dataset_raw: Type[storage_class], transforms: dict, train_flag: bool = True):\n",
        "        \"\"\"\n",
        "        Наследуем весь функционал из `Dataset` для наших данных `dataset_raw`\n",
        "        `transforms` содержит преобразования PIL.Image <-> torch.tensor и аугментации\n",
        "        `train_flag` регулирует аугментацию данных (для тестовой выборки она не нужна)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dataset_raw = dataset_raw\n",
        "        self.transforms = transforms\n",
        "        self.train_flag = train_flag\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_raw.dataset_size\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[Image, Image]:\n",
        "        input, target = self.dataset_raw[idx]\n",
        "\n",
        "        if (self.train_flag):\n",
        "            input, target = self.transforms[\"augment\"]((input, target))\n",
        "\n",
        "        return self.transforms[\"forward_input\"](input), self.transforms[\"forward_target\"](target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCtfvAEKcHLG"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# Разделяем обучающую выборку на обучающую и валидационную\n",
        "def split_train_val(train_data: Type[storage_class], train_portion: float = 0.8):\n",
        "    \"\"\"\n",
        "    `train_data` предобработанные данные\n",
        "    `train_portion` доля объектов, которая будет приходиться на обучающую выборку\n",
        "    \"\"\"\n",
        "    trainval_dataset = SegmentationDataset(train_data, t_dict, train_flag=True)\n",
        "\n",
        "    train_size = int(len(trainval_dataset) * train_portion)\n",
        "    val_size = len(trainval_dataset) - train_size\n",
        "    return random_split(trainval_dataset, [train_size, val_size])\n",
        "\n",
        "train_dataset_hdf5, val_dataset_hdf5 = split_train_val(train_data_hdf5)\n",
        "train_dataset_memmap, val_dataset_memmap = split_train_val(train_data_memmap)\n",
        "train_dataset_raw, val_dataset_raw = split_train_val(train_data_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-GRTgZScHLG"
      },
      "source": [
        "Отрисуем случайное изображение (после применения случайных преобразований аугментации):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiXQ7e-5cHLG"
      },
      "outputs": [],
      "source": [
        "img_idx = np.random.randint(0, 100)\n",
        "draw(train_dataset_hdf5[img_idx], t_dict);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fId8FOvbcHLH"
      },
      "outputs": [],
      "source": [
        "dataloader_config = {\n",
        "    \"batch_size\": 16, # Ваше значение\n",
        "    \"shuffle\": True,\n",
        "    \"num_workers\": 0\n",
        "}\n",
        "train_dataloader_hdf5 = DataLoader(train_dataset_hdf5, **dataloader_config)\n",
        "val_dataloader_hdf5 = DataLoader(val_dataset_hdf5, **dataloader_config)\n",
        "\n",
        "train_dataloader_memmap = DataLoader(train_dataset_memmap, **dataloader_config)\n",
        "val_dataloader_memmap = DataLoader(val_dataset_memmap, **dataloader_config)\n",
        "\n",
        "train_dataloader_raw = DataLoader(train_dataset_raw, **dataloader_config)\n",
        "val_dataloader_raw = DataLoader(val_dataset_raw, **dataloader_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NUMKf-IcHLH"
      },
      "source": [
        "### 1.3 Замер скорости чтения датасета с диска (0.5 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qz0DGOrcHLH"
      },
      "source": [
        "Замерьте время чтения нашего датасета для каждого из форматов хранения:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yzq_d01zcHLH"
      },
      "outputs": [],
      "source": [
        "def speedtest(dataloader: Type[DataLoader]) -> None:\n",
        "    for batch in dataloader:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHr2shN4cHLH"
      },
      "outputs": [],
      "source": [
        "%timeit speedtest(train_dataloader_hdf5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtc2Waf2cHLH"
      },
      "outputs": [],
      "source": [
        "%timeit speedtest(train_dataloader_memmap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0gmLNghcHLI"
      },
      "outputs": [],
      "source": [
        "%timeit speedtest(train_dataloader_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rpo-VN8cHLI"
      },
      "source": [
        "Ответьте на вопрос: `Какой формат оказался самым эффективным по скорости? Почему?`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCqcr5BvcHLI"
      },
      "source": [
        "Ваш ответ: ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVYjAleBcHLI"
      },
      "source": [
        "Создайте тестовый Dataloader победившего по скорости формата."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxSNahaJcHLI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "==== YOUR CODE =====\n",
        "     ¯\\_(ツ)_/¯\n",
        "\"\"\"\n",
        "dataloader_config = {\n",
        "    \"batch_size\": 16, # Ваше значение\n",
        "    \"shuffle\": True,\n",
        "    \"num_workers\": 0\n",
        "}\n",
        "test_dataloader = DataLoader(<sample>, **dataloader_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov09rB71cHLI"
      },
      "source": [
        "## Часть 2: Реализация функций потерь, метрик и декодировщика PSPNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZzYyqFNcHLJ"
      },
      "source": [
        "Ранее вас познакомили с архитектурой Unet - сверточными автокодировщиком, применяемом в области сегментации изображений. В данном задании мы разберем более продвинутую архитектуру сети сегментации [PSPNet](https://arxiv.org/abs/1612.01105). Отличительной особенностью этой сети является `Pyramid Pooling Module`, который, в отличие от Unet, позволяет учитывать `глобальный` контекст изображения при формировании признаков его `локальных` областей."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjBk-dsncHLJ"
      },
      "source": [
        "Рассмотрим предлагаемую архитектуру `PSPNet-подобной сети`:\n",
        "![Image](Architecture.svg)\n",
        "\n",
        "В качестве кодировщика `Encoder` будем брать предобученную [ResNeXt](https://pytorch.org/hub/pytorch_vision_resnext/) сеть. Будем его использовать для получения двух глубинных представлений нашего входого изображения `x`:\n",
        "- выход `x_main` - \"среднее\" промежуточное представление, компромисс между низкоуровневыми признаками (цвет, контуры объектов, штрихи) и высокоуровневыми признаками (абстрактные признаки, отражающие семантику изображения)\n",
        "- выход `x_supp` - финальное представление, содержащее самые высокоуровневые признаки, в которых значительно утеряна информация о точном простанственном расположении объектов\n",
        "\n",
        "Подобное разбиение выхода на 2 потока объясняется необходимостью в закодированной информации о пространственном расположении объектов (`x_main`) и вспомогательной информации о семантике всего изображения в целом (`x_supp`) для задачи семантической сегментации. Мы не можем себе позволить использовать лишь выход `x_supp`, как это делается, например, в задачах классификации, ведь от нас требуется дополнительное знание о расположении этого объекта на изображении.\n",
        "\n",
        "Ваша задача состоит в написании декодировщика `Decoder`, а именно в написании блоков:\n",
        "- `Pyramid Pooling Module`. К входному тензору `x_main` параллельно применяется несколько операций пулинга разных размеров, которые сводят пространственные размерности исходного тензора до размеров `1x1`, `2x2`, `3x3` и `6x6`. Каналы промежуточных тензоров эффективно редуцируются (при помощи `nn.Conv2d c размером фильтра 1x1`), а затем пространственные размерности интерполируются до исходных размеров. Эта процедура необходима для извлечения глобального контекста разных масштабов, которого не хватает классическим сверточным нейронным сетям (локальный контекст в пределах размера фильтра). Таким образом, выходной тензор, полученный конкатенацией этих глобальных контекстов, содержит информацию о всем входном тензоре с разными уровнями детализации. Промежуточное редуцирование каналов тензоров производится для сжатия информации, а также для индивидуального взвешивания глобального контекста каждого масштаба. Требуется реализовать `forward` этап этого блока. Для уточнения информации можно обратиться к [статье](https://arxiv.org/abs/1612.01105).\n",
        "- `Supplementary Module` осуществляет нелинейное преобразование над входным тензором `x_supp` с понижением числа каналов до размерности выхода модуля `Pyramid Pooling Module`. Вариант архитектуры этого преобразования (композиции слоев) уже предложен, но, при желании, вы можете с ним экспериментировать\n",
        "- `Upsample Module` осуществляет нелинейные преобразования над входным тензором с понижением числа каналов, которые чередуются с интерполяцией пространственных размерностей в 2 раза (увеличение). Таким образом, выход этого блока имеет ту же пространственную размерность, что и входное в кодировщик изображение. Это преобразование (слои `Layer 0`, `Layer 1` и `Layer 2`) требуется экспериментально подобрать\n",
        "- `Segmentation Head` нелинейно преобразует входной тензор в тензор score'ов. Имеем, что выходной тензор для каждого пикселя имеет `num_classes` score'ов (в нашем случае 3). В дальнейшем, индекс максимального score'а для заданного пикселя и будет его меткой класса (0, 1 или 2). Это преобразование (композиция слоев) требуется экспериментально подобрать\n",
        "\n",
        "Если декодировщик получается слишком тяжелый, то оператор `Concat` можно заменить на поканальное суммирование. Обратите внимание, что нет единственной правильной архитектуры указанных выше блоков. Требуется ее экспериментально подобрать так, чтобы получить наилучшее качество сегментации за разумную сложность."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og9aaBvOcHLJ"
      },
      "source": [
        "### 2.1 Кодировщик и декодировщик PSPNet-подобной сети (2.5 балла)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-6Ird_wcHLJ"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.resnet import ResNet\n",
        "\n",
        "pretrained_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext50_32x4d', pretrained=True)\n",
        "\n",
        "# Выставляем evaluation mode (влияет на поведение таких слоев как BatchNorm2d, Dropout)\n",
        "pretrained_model.eval();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "019fM1k_cHLJ"
      },
      "source": [
        "Так как кодировщик используется предобученный, то требуется зафиксировать (заморозить) веса, чтобы по ним не тек градиент. Этим мы гарантируем, что кодировщик не изменяется в ходе обучения автокодировщика, а также экономим вычислительные ресурсы (граф градиента кодировщика не строится)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm4LpZ9GcHLK"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, pretrained_model: Type[ResNet]):\n",
        "        \"\"\"\n",
        "        Извлекает предобученные именованные слои кодировщика `pretrained_model`\n",
        "        Разделяет слои на `main` и `supp` потоки (см. архитектуру выше)\n",
        "\n",
        "        Вход: тензор (Batch_size, 3, Height, Width)\n",
        "\n",
        "        Выход: x_main тензор (Batch_size, 512, Height // 8, Width // 8)\n",
        "        Выход: x_supp тензор (Batch_size, 2048, Height // 32, Width // 32)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder_main = nn.Sequential()\n",
        "        for name, child in list(pretrained_model.named_children())[:-4]:\n",
        "            print(f\"Pretrained main module {name} is loaded\")\n",
        "            self.encoder_main.add_module(name, child)\n",
        "\n",
        "        self.encoder_supp = nn.Sequential()\n",
        "        for name, child in list(pretrained_model.named_children())[-4:-2]:\n",
        "            print(f\"Pretrained supp module {name} is loaded\")\n",
        "            self.encoder_supp.add_module(name, child)\n",
        "\n",
        "    def freeze(self) -> None:\n",
        "        \"\"\"\n",
        "        Замораживает веса кодировщика\n",
        "        \"\"\"\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.eval()\n",
        "\n",
        "    def unfreeze(self) -> None:\n",
        "        \"\"\"\n",
        "        Размораживает веса кодировщика\n",
        "        \"\"\"\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = True\n",
        "        self.train()\n",
        "\n",
        "    def forward(self, x: torch.tensor) -> tuple[torch.tensor, torch.tensor]:\n",
        "        x_main = self.encoder_main(x)\n",
        "        x_supp = self.encoder_supp(x_main)\n",
        "        return x_main, x_supp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHLzGIescHLK"
      },
      "outputs": [],
      "source": [
        "encoder = EncoderBlock(pretrained_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1bPva-HcHLK"
      },
      "source": [
        "Для оценки сложности модели нам понадобится функция подсчета числа ее параметров, для этого используйте метод `.parameters()`. Реализуйте ее ниже:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0egKJRgcHLK"
      },
      "source": [
        "Также полезно было бы убедиться, что метод `.parameters()` возвращает то, что мы от него ожидаем. Для этого воспользуемся методом `.named_parameters()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNbaj370cHLK"
      },
      "outputs": [],
      "source": [
        "for name, parameter in encoder.named_parameters():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyPwuYWrcHLL"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model: Type[nn.Module]) -> int:\n",
        "    \"\"\"\n",
        "    Считает число весов в модели `model`, для которых требуется градиент\n",
        "    \"\"\"\n",
        "    total_params = 0\n",
        "    \"\"\"\n",
        "    ==== YOUR CODE =====\n",
        "         ¯\\_(ツ)_/¯\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return total_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMcmyqAncHLL"
      },
      "source": [
        "Убедимся, что метод `.freeze()` успешно замораживает веса:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQe5_MmtcHLL"
      },
      "outputs": [],
      "source": [
        "print(\"Encoder #parameters before freeze():\", count_parameters(encoder))\n",
        "encoder.freeze()\n",
        "print(\"Encoder #parameters after freeze():\", count_parameters(encoder))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHDhFAaxcHLL"
      },
      "source": [
        "Реализуйте `PyramidPoolingModule`, `Upsample` и `SegmentationHead` (по 0.5 балла), а также заполните пропущенные значения `???` в `UpsampleModule` и `DecoderBlock` (по 0.25 балла). Выбор параметров/архитектуры сети в большей степени зависит от результатов обучения в следующей части задания (так что вы еще вернетесь к этому пункту)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2OfQNvkcHLL"
      },
      "outputs": [],
      "source": [
        "class PyramidPoolingModule(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, bin_sizes: tuple[int, ...]):\n",
        "        \"\"\"\n",
        "        Вход: тензор (Batch_size, `in_channels`, Height, Width)\n",
        "        `bin_sizes` - пространственные размерности для каждой пулинг операции\n",
        "        Пример: bin_sizes = (1, 2, 3, 6).\n",
        "\n",
        "        Выход: тензор (Batch_size, `in_channels` + len(`bin_sizes`) * `out_channels`, Height, Width)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.bins = []\n",
        "\n",
        "        for bin_size in bin_sizes:\n",
        "            self.bins.append(nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d(bin_size),\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ))\n",
        "\n",
        "        self.bins = nn.ModuleList(self.bins)\n",
        "\n",
        "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
        "        h, w = x.shape[2:]\n",
        "        out = []\n",
        "        \"\"\"\n",
        "        Осуществите все пулинг-операции с последующим `Upscale`\n",
        "        Подсказка: используйте torch.functional.interpolate\n",
        "        ==== YOUR CODE =====\n",
        "             ¯\\_(ツ)_/¯\n",
        "        \"\"\"\n",
        "        return torch.cat([x] + out, dim=1)\n",
        "\n",
        "\n",
        "class SupplementaryModule(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, dropout: float):\n",
        "        \"\"\"\n",
        "        Вход: тензор (Batch_size, `in_channels`, Height, Width)\n",
        "\n",
        "        Выход: тензор (Batch_size, `out_channels`, Height, Width)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.suppl = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels // 2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(in_channels // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(p=dropout),\n",
        "            nn.Conv2d(in_channels // 2, out_channels, kernel_size=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        \"\"\"\n",
        "        Указанную выше архитектуру можно менять по своему усмотрению\n",
        "        \"\"\"\n",
        "\n",
        "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
        "        return self.suppl(x)\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        \"\"\"\n",
        "        Вход: тензор (Batch_size, `in_channels`, Height, Width)\n",
        "\n",
        "        Выход: тензор (Batch_size, `out_channels`, 2 * Height, 2 * Width)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.us_transform = nn.Sequential(\n",
        "            \"\"\"\n",
        "            ==== YOUR CODE =====\n",
        "                 ¯\\_(ツ)_/¯\n",
        "            \"\"\",\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
        "        \"\"\"\n",
        "        Подсказка: используйте torch.functional.interpolate для удвоения пространственных размерностей\n",
        "        ==== YOUR CODE =====\n",
        "             ¯\\_(ツ)_/¯\n",
        "        \"\"\"\n",
        "        return\n",
        "\n",
        "\n",
        "class UpsampleModule(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int):\n",
        "        \"\"\"\n",
        "        Вход: тензор (Batch_size, `in_channels`, Height, Width)\n",
        "\n",
        "        Выход: тензор (Batch_size, `out_channels`, 8 * Height, 8 * Width)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.upsample = nn.Sequential(\n",
        "            Upsample(in_channels, ???),\n",
        "            Upsample(???, ???),\n",
        "            Upsample(???, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
        "        return self.upsample(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUdJxN48cHLM"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, bin_sizes: tuple[int, ...], dropout: float = 0.1):\n",
        "        \"\"\"\n",
        "        Вход  x_main: тензор (Batch_size, `in_channels`, Height, Width)\n",
        "        Вход  x_supp: тензор (Batch_size, 4 * `in_channels`, Height // 4, Width // 4)\n",
        "\n",
        "        Выход: тензор (Batch_size, `out_channels`, 8 * Height, 8 * Width)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert in_channels % len(bin_sizes) == 0\n",
        "\n",
        "        self.PPM = PyramidPoolingModule(in_channels, ???, bin_sizes)\n",
        "        self.SM = SupplementaryModule(4 * in_channels, ???, dropout)\n",
        "        self.UM = UpsampleModule(???, out_channels)\n",
        "\n",
        "    def forward(self, x_main: torch.tensor, x_supp: torch.tensor) -> torch.tensor:\n",
        "        h_supp, w_supp = x_supp.shape[2:]\n",
        "        x_supp = F.interpolate(input=x_supp, size=(4 * h_supp, 4 * w_supp), mode='bilinear', align_corners=True)\n",
        "\n",
        "        x_supp = self.SM(x_supp)\n",
        "        x_main = self.PPM(x_main)\n",
        "\n",
        "        out = self.UM(torch.cat([x_main, x_supp], dim=1))\n",
        "        return out\n",
        "\n",
        "\n",
        "class SegmentationHead(nn.Module):\n",
        "    def __init__(self, in_channels: int, num_classes: int, dropout: float):\n",
        "        \"\"\"\n",
        "        Вычисляет score для каждого из классов\n",
        "        Вход: тензор (Batch_size, `in_channels`, Height, Width)\n",
        "\n",
        "        Выход: тензор (Batch_size, `num_classes`, Height, Width)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.segmentation_head = nn.Sequential(\n",
        "            \"\"\"\n",
        "            ==== YOUR CODE =====\n",
        "                 ¯\\_(ツ)_/¯\n",
        "            \"\"\",\n",
        "            nn.Conv2d(???, num_classes, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.tensor, x_supp: torch.tensor) -> torch.tensor:\n",
        "        \"\"\"\n",
        "        На будущее зададим фиктивный аргумент `x_supp`, который пока не будем использовать\n",
        "        \"\"\"\n",
        "        return self.segmentation_head(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz2dQNzJcHLM"
      },
      "source": [
        "### 2.2 Реализация метрик (3.5 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFbjxrUacHLM"
      },
      "source": [
        "В задаче сегментации для оценки предсказательной способности нейронной сети, в основном, используют следующие метрики:\n",
        "\n",
        "Пусть $\\mathrm{P}$ обозначает прогноз сег. маски (Prediction), $\\mathrm{S}$ обозначает score'ы для каждого класса сег. маски (Scores), а $\\mathrm{T}$ означает сег. маску (Target). Тогда:\n",
        "- Intersection over Union metric (коэффициент Жаккара):\n",
        "$$\n",
        "\\mathrm{IoU}(P, T) = \\dfrac{\\sum_{i=1}^{M}\\sum_{j=1}^{N}[P_{ij}*T_{ij}]}{\\sum_{i=1}^{M}\\sum_{j=1}^{N} [P_{ij} + T_{ij} - P_{ij}*T_{ij}]}\\text{, где } P, T \\in \\{0, 1\\}^{M \\times N}\n",
        "$$\n",
        "- Recall metric (полнота):\n",
        "$$\n",
        "\\mathrm{Recall}(P, T) = \\dfrac{\\sum_{i=1}^{M}\\sum_{j=1}^{N}[P_{ij} * T_{ij}]}{\\sum_{i=1}^{M}\\sum_{j=1}^{N} T_{ij}}\\text{, где } P, T \\in \\{0, 1\\}^{M \\times N}\n",
        "$$\n",
        "Указанные выше метрики расписаны для случая бинарной сегментации, которая нам не подходит. Обобщим их на случай мультиклассовой сегментации: представим K-классовую задачу как K двухклассовых, а затем `макро-` или `микро-`усредним для них метрики. Требуется реализовать мультиклассовые варианты указанных метрик с поддержкой макро- и микро-усреднения (по 1 баллу). Обратите внимание, что метрики рассчитываются для каждого элемента из батча. За редуцирование метрик вдоль размерности батча отвечает аргумент `reduce` (см. ниже).\n",
        "\n",
        "Также для обучения будем использовать две разные, но схожие функции потерь:\n",
        "- Cross Entropy Loss (кросс-энтропия):\n",
        "$$\n",
        "\\mathrm{CE}(S, T) = - \\dfrac{1}{MN}\\sum_{c=1}^{K}\\sum_{i=1}^{M}\\sum_{j=1}^{N} \\big[\\log \\mathrm{Softmax}(S)_{cij}*\\mathbb{I}[T_{ij} == c]\\big]\\text{, где } S \\in \\mathbb{R}^{K \\times M \\times N}, T \\in \\{1, ..., K\\}^{M \\times N}\n",
        "$$\n",
        "- [Focal Loss](https://arxiv.org/abs/1708.02002):\n",
        "$$\n",
        "\\mathrm{FL}(S, T) = - \\dfrac{1}{MN}\\sum_{c=1}^{K}\\sum_{i=1}^{M}\\sum_{j=1}^{N} \\big[(1 - \\mathrm{Softmax}(S)_{cij})^{\\gamma}*\\log \\mathrm{Softmax}(S)_{cij}*\\mathbb{I}[T_{ij} == c]\\big]\\text{, где } S \\in \\mathbb{R}^{K \\times M \\times N}, T \\in \\{1, ..., K\\}^{M \\times N}, \\gamma \\in \\mathbb{R}_{+} - \\text{гиперпараметр}\n",
        "$$\n",
        "\n",
        "Требуется реализовать обе функции потерь. Также всюду необходимо обеспечить корректную обработку значений `ignore_index`, которые в нашем случае равны 255 (не участвуют в расчете метрик/функций потерь). Если представители некоторых классов в $\\mathrm{T}$ отсутствуют, то учитывать эти классы при макро-усреднении не нужно."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3Fp0We_cHLM"
      },
      "outputs": [],
      "source": [
        "class MetricsCollection():\n",
        "    def __init__(self, num_classes: int, ignore_index: int = 255):\n",
        "        self.num_classes = num_classes\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "    def IoUMetric(self, prediction: torch.tensor, target: torch.tensor, average: str = \"macro\", reduce: str = \"mean\") -> Union[torch.tensor, float]:\n",
        "        \"\"\"\n",
        "        `prediction` предсказанная сегментационная маска размера (Batch_size, Height, Width)\n",
        "        `target` истинная сегментационная маска размера (Batch_size, Height, Width)\n",
        "        `average` тип мультклассового усреднения\n",
        "        `reduce` редукция значений метрики вдоль размерности Batch; None - без редукции\n",
        "        \"\"\"\n",
        "        assert average in [\"micro\", \"macro\"]\n",
        "        assert reduce in [\"sum\", \"mean\", \"none\"]\n",
        "\n",
        "        \"\"\"\n",
        "        ==== YOUR CODE =====\n",
        "             ¯\\_(ツ)_/¯\n",
        "        \"\"\"\n",
        "\n",
        "        return\n",
        "\n",
        "    def RecallMetric(self, prediction: torch.tensor, target: torch.tensor, average: str = \"macro\", reduce: str = \"mean\") -> Union[torch.tensor, float]:\n",
        "        \"\"\"\n",
        "        `prediction` предсказанная сегментационная маска размера (Batch_size, Height, Width)\n",
        "        `target` истинная сегментационная маска размера (Batch_size, Height, Width)\n",
        "        `average` тип мультклассового усреднения\n",
        "        `reduce` редукция значений метрики вдоль размерности Batch; None - без редукции\n",
        "        \"\"\"\n",
        "        assert average in [\"micro\", \"macro\"]\n",
        "        assert reduce in [\"sum\", \"mean\", \"none\"]\n",
        "\n",
        "        \"\"\"\n",
        "        ==== YOUR CODE =====\n",
        "             ¯\\_(ツ)_/¯\n",
        "        \"\"\"\n",
        "\n",
        "        return\n",
        "\n",
        "    def FocalLoss(self, scores: torch.tensor, target: torch.tensor, reduce: str = \"mean\", gamma: float = 1.) -> Union[torch.tensor, float]:\n",
        "        \"\"\"\n",
        "        `scores` score'ы каждого класса сегментационной маски размера (Batch_size, num_classes, Height, Width)\n",
        "        `target` истинная сегментационная маска размера (Batch_size, Height, Width)\n",
        "        `reduce` редукция значений функции потерь вдоль размерности Batch; None - без редукции\n",
        "        \"\"\"\n",
        "        assert scores.shape[1] == self.num_classes\n",
        "        assert reduce in [\"sum\", \"mean\", \"none\"]\n",
        "\n",
        "        ce_loss = F.cross_entropy(scores, target, ignore_index=self.ignore_index, reduction=\"none\")\n",
        "        coef = (1 - torch.exp(-ce_loss))**gamma\n",
        "        focal_loss = coef * ce_loss\n",
        "        norm = (focal_loss.numel() - (target == self.ignore_index).sum())\n",
        "\n",
        "        if (reduce == \"sum\"):\n",
        "            return focal_loss.sum() / norm * scores.shape[0]\n",
        "        elif (reduce == \"mean\"):\n",
        "            return focal_loss.sum() / norm\n",
        "        else:\n",
        "            return focal_loss.sum(dim=[1, 2]) / norm * scores.shape[0]\n",
        "\n",
        "    def CrossEntropyLoss(self, scores: torch.tensor, target: torch.tensor, reduce: str = \"mean\") -> Union[torch.tensor, float]:\n",
        "        \"\"\"\n",
        "        `scores` score'ы каждого класса сегментационной маски размера (Batch_size, num_classes, Height, Width)\n",
        "        `target` истинная сегментационная маска размера (Batch_size, Height, Width)\n",
        "        `reduce` редукция значений функции потерь вдоль размерности Batch; None - без редукции\n",
        "        \"\"\"\n",
        "        assert scores.shape[1] == self.num_classes\n",
        "        assert reduce in [\"sum\", \"mean\", \"none\"]\n",
        "\n",
        "        if (reduce == \"sum\"):\n",
        "            return F.cross_entropy(scores, target, ignore_index=self.ignore_index, reduction=\"mean\") * scores.shape[0]\n",
        "        elif (reduce == \"mean\"):\n",
        "            return F.cross_entropy(scores, target, ignore_index=self.ignore_index, reduction=\"mean\")\n",
        "        else:\n",
        "            return F.cross_entropy(scores, target, ignore_index=self.ignore_index, reduction=\"none\")\n",
        "\n",
        "    @classmethod\n",
        "    def ListMetrics(cls):\n",
        "        return [method for method in dir(cls) if (method.endswith(\"Metric\"))]\n",
        "\n",
        "    @classmethod\n",
        "    def ListLosses(cls):\n",
        "        return [method for method in dir(cls) if (method.endswith(\"Loss\"))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQpn9gvQcHLN"
      },
      "outputs": [],
      "source": [
        "metric_class = MetricsCollection(num_classes=3, ignore_index=255)\n",
        "\n",
        "prediction = torch.tensor([[[0, 0, 0, 0], [0, 0, 1, 0], [0, 1, 1, 0], [0, 0, 0, 0]],\n",
        "                           [[0, 0, 0, 0], [0, 2, 2, 0], [0, 2, 0, 0], [0, 0, 0, 0]]])\n",
        "\n",
        "target = torch.tensor([[[0, 0, 0, 0], [0, 1, 255, 0], [0, 1, 255, 0], [0, 0, 0, 0]],\n",
        "                       [[0, 0, 0, 0], [0, 255, 2, 0], [0, 255, 2, 0], [0, 0, 0, 0]]])\n",
        "\n",
        "assert np.isclose(metric_class.RecallMetric(prediction, target, \"micro\", \"mean\").item(), 0.9286, atol=1e-3)\n",
        "assert np.isclose(metric_class.RecallMetric(prediction, target, \"macro\", \"mean\").item(), 0.7500, atol=1e-3)\n",
        "assert np.isclose(metric_class.IoUMetric(prediction, target, \"micro\", \"mean\").item(), 0.8667, atol=1e-3)\n",
        "assert np.isclose(metric_class.IoUMetric(prediction, target, \"macro\", \"mean\").item(), 0.7115, atol=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNgSrtmJcHLN"
      },
      "source": [
        "Ответьте на вопрос (№1): `Что говорит о предсказательной способности нашей сети ситуация: высокий Recall и низкий IoU для некоторого класса? Возможна ли обратная ситуация?`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYns49HzcHLN"
      },
      "source": [
        "Ваш ответ: ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6NTSjN0cHLN"
      },
      "source": [
        "Ответьте на вопрос (№2): `Какой вид усреднения правильней использовать в нашей задаче: макро и микро? Почему?`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjcjN4igcHLN"
      },
      "source": [
        "Ваш ответ: ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lz2_WGKcHLO"
      },
      "source": [
        "Ответьте на вопрос (№3): `В чем преимущество Focal Loss перед Cross Entropy Loss? Что контроллирует гиперпараметр 𝛾 в Focal Loss?`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGkm8v2NcHLO"
      },
      "source": [
        "Ваш ответ: ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zX2os5RcHLO"
      },
      "source": [
        "## Часть 3: Обучение PSPNet, эксперименты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J45SlK-6cHLO"
      },
      "source": [
        "Теперь осталось лишь собрать все написанное ранее воедино и обучить нашу сеть. Чтобы контроллировать процесс обучения нашей сети, будем вычислять усредненные метрики и функции потерь на валидационной выборке. Для удобства отображения информации воспользуемся инструментом `tensorboard`. Для этого заведем объект класса `SummaryWriter`, который создаст и откроет на запись специальный `event` файл для [tensorboard](https://pytorch.org/docs/stable/tensorboard.html). Для визуализации содержимого вводится команда `tensorboard --logdir=<PATH>` в терминале. Если возникла необходимость в мониториге нескольких tensorboard, то каждому из них требуется присвоить свой уникальный порт `--port <PORT>`. [Пример](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/tensorboard_with_pytorch.ipynb#scrollTo=lFKETpE2F2oE) использования tensorboard на Google Colab.\n",
        "\n",
        "Требуется написать методы `train_model` и `test_model`. Вся конфигурация обучения хранится в словаре `train_config`. При желании его можно дополнить чем-то своим.\n",
        "\n",
        "К вашему решению потребуется прикрепить логи tensorboard. Чтобы облегчить процедуру проверки настоятельно рекомендуется пользоваться `inline-tensorboard`:\n",
        "```\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./runs\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c_INhiYcHLO"
      },
      "source": [
        "### 3.1 Реализация процедур обучения/тестирования сети (1 балл)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxiSD40EcHLO"
      },
      "outputs": [],
      "source": [
        "class PSPNet(nn.Module):\n",
        "    def __init__(self, pretrained_model: Type[ResNet], HeadBlock: Type[nn.Module], num_classes: int, train_config: dict, bin_sizes: tuple[int, ...] = (1, 2, 3, 6)):\n",
        "        \"\"\"\n",
        "        `pretrained_model` модель предобученного кодировщика\n",
        "        `Head` класс блока, оценивающего score'ы для каждого класса сегментационной маски\n",
        "        `num_class` число классов сегментации\n",
        "        `train_config` словарь с конфигурацией процесса обучения сети\n",
        "        `bin_sizes` пространственные размеры к которым сводит пулинг в блоке PPM\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.encoder = EncoderBlock(pretrained_model)\n",
        "        self.encoder.freeze()\n",
        "        self.decoder = DecoderBlock(512, ???, bin_sizes)\n",
        "        self.head = HeadBlock(???, num_classes)\n",
        "\n",
        "        self.train_config = train_config\n",
        "        self.metric_class = train_config[\"metric_class\"]\n",
        "        self.optimizer = train_config[\"optimizer\"](self.parameters(), **train_config[\"optimizer_params\"])\n",
        "        self.scheduler = train_config[\"scheduler\"](self.optimizer, **train_config[\"scheduler_params\"])\n",
        "\n",
        "    def forward(self, x: torch.tensor) -> tuple[torch.tensor, torch.tensor]:\n",
        "        # Для гарантии отсутствия градиентов по кодировщику\n",
        "        with torch.no_grad():\n",
        "            x_main, x_supp = self.encoder(x)\n",
        "        out = self.decoder(x_main, x_supp)\n",
        "        out = self.head(out, x_supp)\n",
        "        return out, torch.argmax(out.detach(), dim=1)\n",
        "\n",
        "    def write_val_metrics(self, val_metrics: dict, iter_num: int, norm: float = 1.0) -> None:\n",
        "        \"\"\"\n",
        "        Записывает усредненные значения метрик/функций потерь в tensorboard\n",
        "\n",
        "        `val_metrics` словарь с ключами \"название_метрики/функции потерь\" и их значениями\n",
        "        `iter_num` номер глобальной итерации (по формуле #всего_итераций * номер_эпохи + номер_итерации)\n",
        "        `norm` фактор нормализации; для усреднения равен числу объектов в валидационной выборке\n",
        "        \"\"\"\n",
        "        for method, value in val_metrics.items():\n",
        "            self.train_config[\"writer\"].add_scalar(f\"Mean {method}\", np.round(val_metrics[method].item()/norm, 2), iter_num)\n",
        "\n",
        "    def validate_model(self, val_dataloader: Type[DataLoader], iter_num: int) -> None:\n",
        "        \"\"\"\n",
        "        Валидирует текущую модель и вычисляет соответствующие метрики/функции потерь\n",
        "\n",
        "        `val_dataloader` валидационная выборка\n",
        "        `iter_num` номер глобальной итерации (по формуле #всего_итераций * номер_эпохи + номер_итерации)\n",
        "        \"\"\"\n",
        "        # Выставляет декодировщик в режим валидации (влияет на поведение BatchNorm2d и Dropout)\n",
        "        self.decoder.eval()\n",
        "\n",
        "        # Инициализация словаря метрик/функций потерь\n",
        "        val_metrics = dict([(method, 0.0) for method in (self.metric_class.ListMetrics() + self.metric_class.ListLosses())])\n",
        "\n",
        "        # Обязательно считать с контекстным менеджером torch.no_grad()\n",
        "        # Даже если мы не делаем шаг оптимизации, мы экономим память (не считаем градиенты)\n",
        "        with torch.no_grad():\n",
        "            for input, target in val_dataloader:\n",
        "                scores, prediction = self.forward(input)\n",
        "                for metric in self.metric_class.ListMetrics():\n",
        "                    val_metrics[metric] += getattr(self.metric_class, metric)(prediction, target, reduce=\"sum\")\n",
        "\n",
        "                for loss in self.metric_class.ListLosses():\n",
        "                    val_metrics[loss] += getattr(self.metric_class, loss)(scores, target, reduce=\"sum\")\n",
        "\n",
        "        # Tensorboard также позволяет сохранять визуализацию наших предсказаний в ходе обучения\n",
        "        figure = draw((input[0], target[0]), t_dict, prediction[0], log=True)\n",
        "        self.train_config[\"writer\"].add_figure(\"image/GT/prediction\", figure, iter_num)\n",
        "\n",
        "        self.write_val_metrics(val_metrics, iter_num, norm=len(val_dataloader.dataset))\n",
        "        # Возвращает режим обучения декодировщика\n",
        "        self.decoder.train()\n",
        "\n",
        "    def train_model(self, train_dataloader: Type[DataLoader], val_dataloader: Type[DataLoader]) -> None:\n",
        "        \"\"\"\n",
        "        Обучает модель на обучающей выборке, периодически (периодичность выставляется в train_config) валидирует на валидационной выборке\n",
        "        В конце каждой эпохи сохраняет модель на диск\n",
        "\n",
        "        `train_dataloader` обучающая выборка\n",
        "        `val_dataloader` валидационная выборка\n",
        "        \"\"\"\n",
        "        # Выставляет режим обучения декодировщика\n",
        "        self.decoder.train()\n",
        "\n",
        "        for epoch in range(self.train_config[\"num_epochs\"]):\n",
        "            for iter_num, (input, target) in enumerate(train_dataloader):\n",
        "                self.optimizer.zero_grad()\n",
        "                \"\"\"\n",
        "                ==== YOUR CODE =====\n",
        "                     ¯\\_(ツ)_/¯\n",
        "                \"\"\"\n",
        "\n",
        "                if (iter_num % self.train_config[\"validate_each_iter\"] == 0):\n",
        "                    print(f\"Epoch: {epoch+1}/{self.train_config['num_epochs']} || Iter: {iter_num}/{len(train_dataloader)} || Loss: {loss.item()}\")\n",
        "                    self.validate_model(val_dataloader, epoch * len(train_dataloader) + iter_num)\n",
        "\n",
        "            torch.save(self.state_dict(), self.train_config[\"save_model_path\"] + f\"_{epoch+1}.pth\")\n",
        "\n",
        "    def test_model(self, test_dataloader: Type[DataLoader]) -> tuple[torch.tensor, torch.tensor]:\n",
        "        \"\"\"\n",
        "        Inference модели на тестовой выборке. Возвращает тензор предсказаний сег.масок и тензор истинных сег.масок\n",
        "\n",
        "        `test_dataloader` тестовая выборка\n",
        "        \"\"\"\n",
        "        # Выставляет декодировщик в режим валидации (влияет на поведение BatchNorm2d и Dropout)\n",
        "        self.decoder.eval()\n",
        "        \"\"\"\n",
        "        ==== YOUR CODE =====\n",
        "             ¯\\_(ツ)_/¯\n",
        "        \"\"\"\n",
        "\n",
        "        return dl_prediction, dl_target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VMyYsebcHLP"
      },
      "source": [
        "### 3.2 Обучение PSPNet, эксперименты (6 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gy-BkS-cHLP"
      },
      "source": [
        "Вам приведены начальные значения гиперпараметров сети. Подберите гиперпараметры (если необходимо) и обучите сеть на обе функции потерь `CrossEntropyLoss` и `FocalLoss`. Добейтесь следующих результатов на тестовой выборке хотя бы для одной из них:\n",
        "- `Mean IoU metric` > 0.87\n",
        "- `Mean Recall metric` > 0.96\n",
        "\n",
        "К вашему решению требуется прикрепить логи tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNyiSHizcHLP"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "train_config = {\n",
        "    \"num_epochs\": 5,\n",
        "    \"optimizer\": torch.optim.Adam,\n",
        "    \"optimizer_params\": {\n",
        "        \"lr\": 1e-3,\n",
        "        \"weight_decay\": 1e-5\n",
        "    },\n",
        "    \"loss_fn\": metric_class.CrossEntropyLoss, # or metric_class.FocalLoss\n",
        "    \"scheduler\": StepLR,\n",
        "    \"scheduler_params\": {\n",
        "        \"step_size\": 50,\n",
        "        \"gamma\": 0.85\n",
        "    },\n",
        "    \"validate_each_iter\": 10,\n",
        "    \"writer\": SummaryWriter(comment=\"CEloss\"), #Floss\n",
        "    \"save_model_path\": <PATH>,\n",
        "    \"metric_class\": metric_class\n",
        "}\n",
        "\n",
        "net = PSPNet(pretrained_model, SegmentationHead, num_classes=3, train_config=train_config).to(DEVICE)\n",
        "print(\"#параметров в сети:\", count_parameters(net))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "cpnVtsvRcHLQ"
      },
      "outputs": [],
      "source": [
        "net.train_model(train_dataloader_<YOUR_FORMAT>, val_dataloader_<YOUR_FORMAT>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foWPu3IdcHLQ"
      },
      "source": [
        "Протестируйте обе модели, сравните метрики:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZHjr4PxcHLQ"
      },
      "outputs": [],
      "source": [
        "net.load_state_dict(torch.load(<PATH>))\n",
        "net.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-KXuvE1cHLQ"
      },
      "outputs": [],
      "source": [
        "dl_prediction, dl_target = net.test_model(test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIqFyDNKcHLQ"
      },
      "outputs": [],
      "source": [
        "print(\"Mean IoU metric: \", metric_class.IoUMetric(dl_prediction, dl_target))\n",
        "print(\"Mean Recall metric: \", metric_class.RecallMetric(dl_prediction, dl_target))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf33vAFZcHLR"
      },
      "source": [
        "Примеры работы вами обученной сети:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUVVJDl9cHLR"
      },
      "outputs": [],
      "source": [
        "img_idx = np.random.randint(0, 100)\n",
        "for idx, (input, target) in enumerate(test_dataloader_hdf5):\n",
        "    if (idx < img_idx):\n",
        "        continue\n",
        "    draw((input.squeeze(), target.squeeze()), t_dict, dl_prediction[idx])\n",
        "    plt.pause(0.1)\n",
        "    if (idx == img_idx+2):\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb4qUZzccHLR"
      },
      "source": [
        "Ответьте на вопрос: `Как выбор функции потерь влияет на рассчитываемые метрики в ходе обучения?`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voqUhX8ccHLR"
      },
      "source": [
        "Ваш ответ: ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCjDIav2cHLR"
      },
      "source": [
        "### 3.3 Бонусное задание: Реализация и обучение двуглавой сети (3 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-v1QCTXcHLR"
      },
      "source": [
        "До этого момента мы ни разу не использовали тот факт, что в нашем датасете не бывает слуаев, в которых и собака, и кошка одновременно находятся в кадре. В это же время блок `SegmentationHead` допускает этот случай, что дает теоретическую возможность модели ошибиться. Чтобы повысить устойчивость модели мы будем использовать две головы: `голова двухклассовой сегментации`, которая сегментирует животное на изображении, а вторая `голова бинарной классификации` будет предсказывать, что это за животное (собака или кошка). Таким образом, наша модель не имеет возможности отнести голову животного к классу \"собака\", а туловище к классу \"кошка\", что увеличивает ее устойчивость. Реализуйте двуглавый блок `SegmentationClassificationHeads`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtgNbZGEcHLR"
      },
      "outputs": [],
      "source": [
        "class SegmentationClassificationHeads(nn.Module):\n",
        "    def __init__(self, in_channels: int, num_classes: int, dropout: float = 0.1):\n",
        "        \"\"\"\n",
        "        Вычисляет score для каждого из классов\n",
        "        Вход: тензор (Batch_size, `in_channels`, Height, Width)\n",
        "\n",
        "        Выход: тензор (Batch_size, `num_classes`, Height, Width)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.segmentation_head = nn.Sequential(\n",
        "            \"\"\"\n",
        "            ==== YOUR CODE =====\n",
        "                 ¯\\_(ツ)_/¯\n",
        "            \"\"\"\n",
        "            nn.Conv2d(???, 2, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self.classification_head = nn.Sequential(\n",
        "            \"\"\"\n",
        "            ==== YOUR CODE =====\n",
        "                 ¯\\_(ツ)_/¯\n",
        "            \"\"\"\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(???, num_classes - 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def combine_heads(self, seg_pred: torch.tensor, cls_pred: torch.tensor) -> torch.tensor:\n",
        "        \"\"\"\n",
        "        ==== YOUR CODE =====\n",
        "             ¯\\_(ツ)_/¯\n",
        "        \"\"\"\n",
        "        return\n",
        "\n",
        "    def forward(self, x: torch.tensor, x_supp: torch.tensor) -> torch.tensor:\n",
        "        \"\"\"\n",
        "        Вот мы и воспользовались ранее фиктивным аргументом `x_supp`\n",
        "        \"\"\"\n",
        "        cls_pred = self.classification_head(x_supp)\n",
        "        seg_pred = self.segmentation_head(x)\n",
        "        return self.combine_heads(seg_pred, cls_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yusq9sSAcHLS"
      },
      "source": [
        "Обучите двуглавую сеть и получите улучшение метрик относительно наилучшего результата предыдущего пункта:\n",
        "- `Mean IoU metric` > 0.93\n",
        "- `Mean Recall metric` > 0.96\n",
        "\n",
        "К вашему решению требуется прикрепить логи tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM0OBsdlcHLS"
      },
      "outputs": [],
      "source": [
        "train_config[\"writer\"] = SummaryWriter(comment=\"TwoHead_CEloss\") #TwoHead_Floss\n",
        "train_config[\"save_model_path\"] = <PATH>\n",
        "\n",
        "net = PSPNet(pretrained_model, SegmentationClassificationHeads, num_classes=3, train_config=train_config).to(DEVICE)\n",
        "print(\"#параметров в сети:\", count_parameters(net))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "KZ6WAeXzcHLS"
      },
      "outputs": [],
      "source": [
        "net.train_model(train_dataloader_<YOUR_FORMAT>, val_dataloader_<YOUR_FORMAT>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWJurQcfcHLS"
      },
      "source": [
        "Тестируем модель:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBYM-hamcHLS"
      },
      "outputs": [],
      "source": [
        "net.load_state_dict(torch.load(<PATH>))\n",
        "net.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpZZ5-4ycHLS"
      },
      "outputs": [],
      "source": [
        "dl_prediction, dl_target = net.test_model(test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jqHwODwcHLS"
      },
      "outputs": [],
      "source": [
        "print(\"Mean IoU metric: \", metric_class.IoUMetric(dl_prediction, dl_target))\n",
        "print(\"Mean Recall metric: \", metric_class.RecallMetric(dl_prediction, dl_target))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyp6Lfg1cHLT"
      },
      "source": [
        "Примеры работы вами обученной двуглавой сети:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ns-NXNT8cHLT"
      },
      "outputs": [],
      "source": [
        "img_idx = np.random.randint(0, 100)\n",
        "for idx, (input, target) in enumerate(test_dataloader_hdf5):\n",
        "    if (idx < img_idx):\n",
        "        continue\n",
        "    draw((input.squeeze(), target.squeeze()), t_dict, dl_prediction[idx])\n",
        "    plt.pause(0.1)\n",
        "    if (idx == img_idx+2):\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE5n9covcHLT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rz2dQNzJcHLM",
        "5c_INhiYcHLO",
        "8VMyYsebcHLP",
        "JCjDIav2cHLR"
      ]
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "49f33d7bf55c439cbdd37c1f10bab439": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a44f67409424ca291f50cfe02e4770e",
              "IPY_MODEL_89d3d1f29b564a10bd0747e8f30a6f04",
              "IPY_MODEL_c18c8b4c00f44b0c8ed74fd9324be686"
            ],
            "layout": "IPY_MODEL_b313786dc33341739d71dd85d95b1625"
          }
        },
        "8a44f67409424ca291f50cfe02e4770e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb9d93da51e3458d9cf80985d239db06",
            "placeholder": "​",
            "style": "IPY_MODEL_0b9eeb5ab4624ec39e8d18b6bbcb7888",
            "value": "  0%"
          }
        },
        "89d3d1f29b564a10bd0747e8f30a6f04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14eee69c1bcf4430892ee14289e09132",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb6401b48b254c6590ce91e799df1d20",
            "value": 0
          }
        },
        "c18c8b4c00f44b0c8ed74fd9324be686": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4393105203d489cac0a2854efddac94",
            "placeholder": "​",
            "style": "IPY_MODEL_ea207ac9769a47e79bdded2974a3ae94",
            "value": " 0/8 [00:24&lt;?, ?it/s]"
          }
        },
        "b313786dc33341739d71dd85d95b1625": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb9d93da51e3458d9cf80985d239db06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b9eeb5ab4624ec39e8d18b6bbcb7888": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14eee69c1bcf4430892ee14289e09132": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb6401b48b254c6590ce91e799df1d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4393105203d489cac0a2854efddac94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea207ac9769a47e79bdded2974a3ae94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}